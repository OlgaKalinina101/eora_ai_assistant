import re
import warnings
from typing import List, Set, Dict

from chromadb.api.models import Collection
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

from data_extraction.dataset_builder import build_cases_dataset
from data_ingestion.ingestor import KnowledgeBaseBuilder
from settings import settings
from utils.chroma_client import get_chroma_client
from utils.logger import setup_logger

# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è torch
warnings.filterwarnings(
    "ignore",
    category=FutureWarning,
    message="`encoder_attention_mask` is deprecated",
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
logger = setup_logger("chunks")

def find_relevant_chunks(
    question: str,
    collection: Collection,
    embedder: SentenceTransformer,
    top_k: int = 10,
) -> List[str]:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

    Args:
        question: –°–µ–≥–º–µ–Ω—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ß—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å –¥–ª—è —Ä–∏—Ç–µ–π–ª–µ—Ä–æ–≤?").
        collection: –ö–æ–ª–ª–µ–∫—Ü–∏—è ChromaDB.
        embedder: –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (SentenceTransformer).
        top_k: –°–∫–æ–ª—å–∫–æ —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤ –≤–µ—Ä–Ω—É—Ç—å.

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if not question.strip():
        logger.warning("–ü—É—Å—Ç–æ–π —Å–µ–≥–º–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.")
        return []
    if top_k <= 0:
        logger.warning(f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ top_k ({top_k}), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.")
        return []

    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞: –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –ø—É—Å—Ç–∞
        if collection.count() == 0:
            logger.warning("üîÑ –ö–æ–ª–ª–µ–∫—Ü–∏—è Chroma –ø—É—Å—Ç–∞. –ó–∞–ø—É—Å–∫–∞—é –ø–µ—Ä–µ—Å–±–æ—Ä–∫—É –±–∞–∑—ã...")

            # –®–∞–≥ 1: –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            build_cases_dataset(settings.PDF_PATH, settings.OUTPUT_JSON)
            logger.info("üîÑ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")

            # –®–∞–≥ 2: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            builder = KnowledgeBaseBuilder()
            builder.ingest()
            logger.info("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞.")

            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º collection, —á—Ç–æ–±—ã –æ–Ω–∞ —É–≤–∏–¥–µ–ª–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            collection = get_chroma_client().get_or_create_collection(settings.CHROMA_COLLECTION_NAME)

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏ –ø–æ–∏—Å–∫
        query_embedding = embedder.encode(question, normalize_embeddings=True)
        results = collection.query(query_embeddings=[query_embedding], n_results=top_k)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        documents = results.get("documents", [[]])[0]
        metadatas = results.get("metadatas", [[]])[0]
        distances = results["distances"][0]

        # –°–∫–ª–µ–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç, source –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é
        max_distance = 1.3  # –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º —á–µ—Ä–µ–∑ settings
        chunks_with_sources = [
            {"text": doc, "source": meta.get("source", "unknown")}
            for doc, meta, dist in zip(documents, metadatas, distances)
            if dist <= max_distance
        ]

        filtered_chunks = rerank_by_tfidf(chunks_with_sources, question)
        logger.info(f"üîé –ù–∞–π–¥–µ–Ω–æ {len(filtered_chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ —Å–µ–≥–º–µ–Ω—Ç—É '{question}' (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫).")

        return filtered_chunks

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø–æ–∏—Å–∫–µ: {e}")
        return []


def rerank_by_tfidf(
    filtered_chunks: List[Dict[str, str]], question: str, top_k: int = 3
) -> List[Dict[str, str]]:
    """
    –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        filtered_chunks (List[Dict[str, str]]): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏,
            –∫–∞–∂–¥—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á 'text' —Å —Ç–µ–∫—Å—Ç–æ–º.
        question (str): –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
        top_k (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.
            –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 3.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        List[Dict[str, str]]: –°–ø–∏—Å–æ–∫ —Ç–æ–ø-k —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —É–±—ã–≤–∞–Ω–∏—é
            —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.

    –ò—Å–∫–ª—é—á–µ–Ω–∏—è:
        ValueError: –ï—Å–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã (–ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤,
            –ø—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π top_k).
    """
    if not filtered_chunks or not all(isinstance(chunk, dict) and 'text' in chunk for chunk in filtered_chunks):
        raise ValueError('filtered_chunks –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–æ–º "text"')
    if not question or not isinstance(question, str):
        raise ValueError('question –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π')
    if not isinstance(top_k, int) or top_k < 1:
        raise ValueError('top_k –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º')

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
    try:
        vectorizer = TfidfVectorizer(stop_words=None)
        tfidf_matrix = vectorizer.fit_transform(
            chunk['text'] for chunk in filtered_chunks
        )
        feature_names = vectorizer.get_feature_names_out()
    except Exception as e:
        raise ValueError(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ TF-IDF: {str(e)}')

    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
    query_keywords: Set[str] = set(re.findall(r'\w+', question.lower()))

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
    scored_docs: List[tuple[float, Dict[str, str]]] = []
    for idx, doc in enumerate(filtered_chunks):
        tfidf_vector = tfidf_matrix[idx].toarray()[0]
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ –∑–∞–ø—Ä–æ—Å–µ
        score = sum(
            tfidf_vector[i]
            for i in tfidf_vector.nonzero()[0]
            if feature_names[i] in query_keywords
        )
        if score > 0:
            scored_docs.append((score, doc))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–µ—Å–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k
    scored_docs.sort(key=lambda x: x[0], reverse=True)
    return [doc for _, doc in scored_docs[:top_k]]



